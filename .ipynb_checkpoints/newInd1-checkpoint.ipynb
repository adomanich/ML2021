{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define dependencies.\n",
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import seaborn as sn\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3b42916d62ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Import datasets.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrestaurant_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrestaurant_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test.csv'"
     ]
    }
   ],
   "source": [
    "#Import datasets.\n",
    "restaurant_train = pd.read_csv('train.csv')\n",
    "restaurant_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see what columns we have available, and if they coincide with those provided on kaggle.\n",
    "restaurant_train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#head view of dataset\n",
    "restaurant_train.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Revenue generating cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter dataset to city and revenue columns.\n",
    "rev_city = restaurant_train[['City','revenue']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a list of unique cities to be used for the revenue filter.\n",
    "city = rev_city['City'].drop_duplicates().sort_values().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group revenues by city.\n",
    "tcity = rev_city.groupby('City')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate list of revenues for each unique city.\n",
    "rev_city = [sum(tcity.get_group(i)['revenue']) for i in city]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate horizontal bar plot to see which city generates the most revenue.\n",
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "y_pos = np.arange(len(city))\n",
    "\n",
    "ax.barh(y_pos,rev_city, color ='r')\n",
    "ax.set_ylabel('City')\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(city)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Revenue')\n",
    "ax.set_title('Which cities generates the most revenue?')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the necessary columns for this part.\n",
    "rev_type = restaurant_train[['Type','revenue']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consolidate the 'Type' features.\n",
    "rtype = rev_type['Type'].drop_duplicates().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group the values by 'Type'\n",
    "ttype = rev_type.groupby('Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sum and store the revenues by 'Type'\n",
    "rev_type = [sum(ttype.get_group(i)['revenue']) for i in rtype]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the complete 'Type' names.\n",
    "rtype = ['Inline', 'Food Court', 'Drive Thru']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate horizontal bar plot to see which restaurant type generates the most revenue.\n",
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots(figsize=(9,3))\n",
    "y_pos = np.arange(len(rtype))\n",
    "\n",
    "ax.barh(y_pos,rev_type,color='r')\n",
    "ax.set_ylabel('Restaurant Type')\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(rtype)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Revenue')\n",
    "ax.set_title('Which restaurant type generates the most revenue?')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Launch dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate both datasets.\n",
    "tot = restaurant_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store months and years in lists.\n",
    "month = [i.split('/')[0] + '/' for i in tot['Open Date']]\n",
    "year = [i.split('/')[2] for i in tot['Open Date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a new dataframe for this part of the exploration.\n",
    "odate = pd.DataFrame([x+y for x,y in zip(month,year)], columns = ['date']).sort_values(by=['date'])\n",
    "odate = odate.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform 'date' column to datetime format in order to accurately sort the dates, \n",
    "#then send it back as a string.\n",
    "odate['date'] = pd.to_datetime(odate['date'])\n",
    "odate = odate.sort_values(by='date').reset_index(drop=True)\n",
    "odate['date'] = [str(i)[:7] for i in odate['date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the counts for each date considered, preparing it for a bar plot.\n",
    "date_count = [i for i in odate['date'].value_counts().sort_index()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Minimize dataset to one entry per date to match the corresponding counts previously calculated.\n",
    "odate = odate.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the counts into the date subset dataset.\n",
    "odate['date_count'] = date_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the trend in a barplot.\n",
    "plt.figure(figsize=(23,8))\n",
    "plt.bar(odate['date'],odate['date_count'],color='r')\n",
    "plt.title('When were the restaurants mostly launched?')\n",
    "plt.xticks(rotation='45')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if data revenue is normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.probplot(restaurant_train['revenue'], dist = 'norm', plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Seeing that the data has some skewness when plotting the Q-Q plot and some outlier data points. For this reason, a logarithm was applied on the revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.probplot(np.log(restaurant_train['revenue']), dist='norm', plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation before using linear regression for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First column that needs to be transformed from a categorical variable to a numerical one.\n",
    "restaurant_train['City Group'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace the column values with 0 if it is a 'Big City' data point, otherwise place a 1, \n",
    "#and store these values into the same column.\n",
    "restaurant_train['City Group'] = [1 if i == 'Other' else 0 for i in restaurant_train['City Group'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the dummy variables for the 'type' column.\n",
    "restaurant_train['Type'] = [int(3) if k == 'DT' else k for k in [int(2) if j == 'IL' else j for j in [int(1) if i == 'FC' else i for i in restaurant_train['Type']]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#One last thing to consider is the date, \n",
    "#and since this is a string, I will cut it down to just the year and transform it to an int.\n",
    "restaurant_train[['month','day','year']] = restaurant_train['Open Date'].str.split('/',expand=True)\n",
    "restaurant_train['year'] = [int(i) for i in restaurant_train['year']]\n",
    "restaurant_train['month'] = [int(i) for i in restaurant_train['month']]\n",
    "restaurant_train['day'] = [int(i) for i in restaurant_train['day']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset is ready for PCA section and beyond.\n",
    "restaurant_train = restaurant_train.iloc[:,3:]\n",
    "restaurant_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's make a copy of the test set.\n",
    "res_test = restaurant_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's start forming our dataset to match that of the training set, \n",
    "#with the exception of the revenue column for the analysis.\n",
    "res_test[['month','day','year']] = res_test['Open Date'].str.split('/',expand=True)\n",
    "res_test['year'] = [int(i) for i in res_test['year']]\n",
    "res_test['month'] = [int(i) for i in res_test['month']]\n",
    "res_test['day'] = [int(i) for i in res_test['day']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replicate dummy variable for the city group variable.\n",
    "res_test['City Group'] = [1 if i == 'Other' else 0 for i in res_test['City Group']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replicate the dummy variable process for the 'type' column.\n",
    "res_test['Type'] = [int(3) if k == 'DT' else k for k in [int(2) if j == 'IL' else j for j in [int(1) if i == 'FC' else i for i in res_test['Type']]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since this dataset is the one that contains the mobile option (MB), \n",
    "#then I will be setting this as null in order to find and compensate for it in the next steps.\n",
    "res_type = [np.nan if i == 'MB' else i for i in res_test['Type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Substitute back in for the 'type' column in the main dataframe.\n",
    "res_test['Type'] = res_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see what it looks like!\n",
    "knni_test = res_test.iloc[:,3:]\n",
    "knni_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the KNNImputer function.\n",
    "imputer = KNNImputer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Send data to the KNNImputer\n",
    "imputer.fit(knni_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the transformed data once sent to the KNNImputer\n",
    "knni_sol = imputer.transform(knni_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Format the resultant dataframe from the data received.\n",
    "knni_test = pd.DataFrame(knni_sol, columns = [i for i in knni_test.columns.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that the dataframe is updated to compensate for the mobile (MB) option of the 'type' column,\n",
    "#let's quickly see what values are available and if they coincide with those available on the training dataset.\n",
    "knni_test['Type'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iven that the values available are floats with decimal values in between the values allowed,\n",
    "#this will be fixed by setting boundaries that will output corresponding dummy variables to those in the training set.\n",
    "knni_test['Type'] = [2.0 if (j >= 1.5 and j < 2.5) else j for j in [1 if (i > 1 and i < 1.5) else i for i in knni_test['Type']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knni_test.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can now see that the two datasets agree with each other on the variables available, with the exception of the response/target variable, namely 'revenue'. With this in place, the next thing to figure out is to see what variables are vital for the main analysis-predicting revenue-which will be carried out through a Principal Component Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res_train['City'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res_test['City'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Before the Principal Component Analysis (PCA) is performed though, the data needs to be scaled in order for all of the variables to have a more fully balanced dataset that will give the most accurate results. As it was mentioned in the data exploration part though, the 'revenue' target feature seems to be more fully normalized when taking it's logarithm. So, just before this normalization, let's take the logarithm of that column and reinstate it in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtrain_y = restaurant_train['revenue'].copy()\n",
    "rtrain_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take the logarithm of the 'revenue' column to generate better results.\n",
    "restaurant_train['revenue'] = np.log(restaurant_train['revenue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the response variable for the training set.\n",
    "rtrain_ylog = restaurant_train['revenue']\n",
    "rtrain_ylog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update the feature dataset.\n",
    "restaurant_train.drop(columns=['revenue'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a StandardScaler instance.\n",
    "scaler = StandardScaler()\n",
    "rtrain_x = scaler.fit_transform(restaurant_train)\n",
    "rtrain_x = pd.DataFrame(rtrain_x, columns = [i for i in restaurant_train.columns.values])\n",
    "rtrain_x.head()\n",
    "\n",
    "rtest_x = scaler.fit_transform(knni_test)\n",
    "rtest_x = pd.DataFrame(rtest_x, columns = [i for i in knni_test.columns.values])\n",
    "rtest_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "#### Principal Component Analysis\n",
    "Once we have the data normalized, we can perform the PCA to reduce the amount of dimensions available. Using the sci-kit learn library from python, the steps of this process go as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the Principal Component Analysis (PCA) with 10 features as our target.\n",
    "pca_train = PCA(n_components = 10)\n",
    "pca_test = PCA(n_components = 10)\n",
    "\n",
    "#Fit the feature data to the PCA.\n",
    "pca_train.fit(rtrain_x)\n",
    "pca_test.fit(rtest_x\n",
    "             \n",
    "#Let's see how they are weighted.\n",
    "print(pca_train.explained_variance_ratio_)\n",
    "print(pca_test.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fully transform the feature data to the PCA features.\n",
    "pca_train_rcomp = pca_train.transform(rtrain_x)\n",
    "pca_test_rcomp = pca_test.transform(rtest_x)\n",
    "\n",
    "#Generate and display the dataframe with the main PCA components generated.\n",
    "pca_train = pd.DataFrame(data=pca_train_rcomp, columns = ['PCA '+str(i) for i in range(10)])\n",
    "pca_train.head()\n",
    "\n",
    "#Generate and display the dataframe with the main PCA components generated.\n",
    "pca_test = pd.DataFrame(data=pca_test_rcomp, columns = ['PCA '+str(i) for i in range(10)])\n",
    "pca_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression\n",
    "\n",
    "Arriving at the final portion of the analysis, the revenue will be predicted using a linear regression method. Considering the different combinations available for both the response variables and the features available(x), the results in the end speak for themselves and hint at the non-normalized and non-PCA method to be the best performing ð‘…^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choice 1: PCA training set and log applied response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = linear_model.LinearRegression()\n",
    "lr_model.fit(pca_train,rtrain_ylog)\n",
    "y_pred = lr_model.predict(pca_test)\n",
    "\n",
    "print(y_pred)\n",
    "print(lr_model.score(pca_train,rtrain_ylog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choice 2: PCA training set and normal response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_mod1 = linear_model.LinearRegression()\n",
    "lr_mod1.fit(pca_train, rtrain_y)\n",
    "y_pred1 = lr_mod1.predict(pca_test)\n",
    "\n",
    "print(y_pred1)\n",
    "print(lr_mod1.score(pca_train, rtrain_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choice 3: Regular training set and normal response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_mod2 = linear_model.LinearRegression()\n",
    "lr_mod2.fit(restaurant_train, rtrain_y)\n",
    "y_pred2 = lr_mod2.predict(knni_test)\n",
    "\n",
    "print(y_pred2)\n",
    "print(lr_mod2.score(restaurant_train, rtrain_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display predicted values for the 100000 rows in the test set.\n",
    "[i for i in y_pred2]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
